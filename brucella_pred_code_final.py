# -*- coding: utf-8 -*-
"""brucella_pred_code_final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CRDuNp_A42gsKOSHq1WlvIb5jk9JiFy9
"""

# Cell 1: Import Libraries and Setup
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
import pickle # Import pickle for saving objects

warnings.filterwarnings('ignore')

# ML models
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_auc_score

# Balancing
from imblearn.over_sampling import SMOTE, RandomOverSampler

# XGBoost
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
    print("✅ XGBoost is available")
except ImportError:
    XGBOOST_AVAILABLE = False
    print("❌ XGBoost not available, will skip XGBoost model")

print("📚 All libraries imported successfully!")

# Cell 2: Data Loading and Initial Exploration
def load_and_explore_data(filepath):
    """Load and explore the dataset"""
    print("📁 Loading data...")

    # Load data
    df_raw = pd.read_csv(filepath, encoding='ISO-8859-1')
    print(f"📊 Original data shape: {df_raw.shape}")
    print(f"🔍 Columns: {df_raw.columns.tolist()}")

    # Check target column
    target_col = 'Test Result (Positive Negative Inconclusive)'
    if target_col in df_raw.columns:
        print(f"\n🎯 Target column found: {target_col}")
        print("📈 Original class distribution:")
        print(df_raw[target_col].value_counts())
    else:
        print("❌ Target column not found!")
        return None, None

    return df_raw, target_col

# Load the data
df_raw, target_col = load_and_explore_data("Brucellosis_Proforma_Mathura.csv")

# Cell 3: Data Preprocessing
def preprocess_data(df_raw, target_col):
    """Clean and preprocess the data"""
    print("🧹 Preprocessing data...")

    # Drop unnecessary columns
    columns_to_drop = ["Sl No. ", " Animal No. ", "Date Duration of sample collection "]
    df_processed = df_raw.drop(columns=[col for col in columns_to_drop if col in df_raw.columns])

    # Clean target column
    df_processed[target_col] = (
        df_processed[target_col]
        .astype(str)
        .str.strip()
        .str.title()
        .replace(['', 'Nan', 'NaN', 'None', 'Inconclusive'], 'Suspect')  # Map Inconclusive to Suspect
    )

    # Filter for only our three target classes
    valid_classes = ['Positive', 'Negative', 'Suspect']
    df_filtered = df_processed[df_processed[target_col].isin(valid_classes)].copy()

    print(f"📊 Data shape after filtering: {df_filtered.shape}")
    print(f"🎯 Final class distribution:")
    print(df_filtered[target_col].value_counts())

    return df_filtered

# Preprocess the data
df_processed = preprocess_data(df_raw, target_col)

# Cell 4: Handle Missing Values
def handle_missing_values(df, target_col):
    """Handle missing values in the dataset"""
    print("🔧 Handling missing values...")

    # Check missing values
    missing_info = df.isnull().sum()
    print(f"📊 Missing values per column:")
    print(missing_info[missing_info > 0])

    # Separate features and target
    df_features = df.drop(columns=[target_col])
    df_target = df[target_col]

    # Impute missing values in features
    imputer_features = SimpleImputer(strategy='most_frequent')
    df_features_imputed = pd.DataFrame(
        imputer_features.fit_transform(df_features),
        columns=df_features.columns
    )

    # Handle missing target values
    if df_target.isnull().any():
        print("⚠️ Found missing values in target, imputing...")
        imputer_target = SimpleImputer(strategy='most_frequent')
        df_target_imputed = pd.Series(
            imputer_target.fit_transform(df_target.values.reshape(-1, 1)).flatten(),
            index=df_target.index,
            name=target_col
        )
    else:
        df_target_imputed = df_target.copy()

    # Combine features and target
    df_final = df_features_imputed.join(df_target_imputed)
    df_final.columns = df_final.columns.str.strip()

    print("✅ Missing values handled successfully!")
    return df_final

# Handle missing values
df_clean = handle_missing_values(df_processed, target_col)

# Save df_clean.csv
df_clean.to_csv('df_clean.csv', index=False)
print("💾 Saved df_clean.csv")

# Cell 5: Encode Categorical Features
def encode_categorical_features(df, target_col):
    """Encode categorical features"""
    print("🔢 Encoding categorical features...")

    df_encoded = df.copy()
    le_dict = {}

    # Encode categorical features (excluding target)
    categorical_cols = df_encoded.select_dtypes(include='object').columns
    categorical_cols = [col for col in categorical_cols if col != target_col]

    for col in categorical_cols:
        le = LabelEncoder()
        df_encoded[col] = le.fit_transform(df_encoded[col])
        le_dict[col] = le
        print(f"   ✅ Encoded {col}: {len(le.classes_)} unique values")

    # Separate features and target
    X = df_encoded.drop(columns=[target_col])
    y = df_encoded[target_col]

    # Encode target
    le_target = LabelEncoder()
    y_encoded = le_target.fit_transform(y)

    print(f"🎯 Target classes: {le_target.classes_}")
    print(f"📊 Encoded class distribution: {Counter(y_encoded)}")

    return X, y_encoded, le_dict, le_target

# Encode features
X, y, le_dict, le_target = encode_categorical_features(df_clean, target_col)
feature_names = X.columns.tolist()

# Save feature_names.pkl, le_dict.pkl, le_target.pkl
with open('feature_names.pkl', 'wb') as f:
    pickle.dump(feature_names, f)
print("💾 Saved feature_names.pkl")

with open('le_dict.pkl', 'wb') as f:
    pickle.dump(le_dict, f)
print("💾 Saved le_dict.pkl")

with open('le_target.pkl', 'wb') as f:
    pickle.dump(le_target, f)
print("💾 Saved le_target.pkl")

# Cell 6: Data Balancing
def balance_dataset(X, y):
    """Balance the dataset using SMOTE"""
    print("⚖️ Balancing dataset...")

    class_counts = Counter(y)
    minority_class = min(class_counts, key=class_counts.get)
    n_minority = class_counts[minority_class]

    print(f"📊 Class distribution before balancing: {class_counts}")

    if n_minority > 1:
        k = max(1, min(5, n_minority - 1))
        smote = SMOTE(random_state=42, k_neighbors=k)
        X_balanced, y_balanced = smote.fit_resample(X, y)
        print(f"✅ Used SMOTE with k_neighbors={k}")
    else:
        ros = RandomOverSampler(sampling_strategy='auto', random_state=42)
        X_balanced, y_balanced = ros.fit_resample(X, y)
        print("✅ Used RandomOverSampler")

    print(f"📊 Class distribution after balancing: {Counter(y_balanced)}")

    return X_balanced, y_balanced

# Balance the dataset
X_balanced, y_balanced = balance_dataset(X, y)

# Cell 7: Train-Test Split
print("🔄 Splitting data into train and test sets...")

X_train, X_test, y_train, y_test = train_test_split(
    X_balanced, y_balanced,
    test_size=0.2,
    random_state=42,
    stratify=y_balanced
)

print(f"📊 Training set size: {X_train.shape[0]}")
print(f"📊 Test set size: {X_test.shape[0]}")
print(f"📊 Training class distribution: {Counter(y_train)}")
print(f"📊 Test class distribution: {Counter(y_test)}")

# Cell 8: Feature Scaling
print("📏 Scaling features...")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Save scaler.pkl
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)
print("💾 Saved scaler.pkl")

print("✅ Feature scaling completed!")

# Cell 9: Define Model Configurations
def get_model_configs():
    """Get all model configurations"""
    model_configs = {
        "Extra Trees": ExtraTreesClassifier(n_estimators=100, random_state=42),
        "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
        "Decision Tree": DecisionTreeClassifier(random_state=42),
        "MLP": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42),
        "SVM": SVC(kernel='rbf', probability=True, random_state=42),
        "Naive Bayes": GaussianNB(),
        "KNN": KNeighborsClassifier(n_neighbors=5),
        "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
        "Gradient Boosting": GradientBoostingClassifier(random_state=42)
    }

    # Add XGBoost if available
    if XGBOOST_AVAILABLE:
        model_configs["XGBoost"] = XGBClassifier(
            use_label_encoder=False,
            eval_metric='mlogloss',
            random_state=42
        )

    return model_configs

model_configs = get_model_configs()
print(f"🤖 Configured {len(model_configs)} models for training")

# Cell 10: Train Models
def train_single_model(name, model, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled):
    """Train a single model and return results"""
    print(f"    🚀 Training {name}...")

    # Use scaled features for models that benefit from scaling
    if name in ["MLP", "SVM", "Logistic Regression", "KNN"]:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        y_prob = model.predict_proba(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)

    # Calculate AUC for multiclass
    try:
        auc_score = roc_auc_score(y_test, y_prob, multi_class='ovr', average='macro')
    except:
        auc_score = 0.0

    return {
        'model': model,
        'accuracy': accuracy,
        'auc': auc_score,
        'predictions': y_pred,
        'probabilities': y_prob,
        'classification_report': classification_report(y_test, y_pred, output_dict=True)
    }

# Train all models
print("🎯 Training all models...")
model_results = {}

for name, model in model_configs.items():
    result = train_single_model(name, model, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled)
    model_results[name] = result
    print(f"    ✅ {name} - Accuracy: {result['accuracy']:.4f}, AUC: {result['auc']:.4f}")

# Save all_model_results.pkl
with open('all_model_results.pkl', 'wb') as f:
    pickle.dump(model_results, f)
print("💾 Saved all_model_results.pkl")

print(f"\n🏁 Training completed for {len(model_results)} models!")

# Cell 11: Model Performance Analysis
def analyze_model_performance(model_results):
    """Analyze and display model performance"""
    print(f"\n📋 MODEL PERFORMANCE SUMMARY")
    print(f"{'='*60}")
    print(f"{'Model':<20} {'Accuracy':<10} {'AUC':<10}")
    print(f"{'-'*60}")

    for name, results in model_results.items():
        print(f"{name:<20} {results['accuracy']:<10.4f} {results['auc']:<10.4f}")

    # Find best model
    best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['accuracy'])
    best_accuracy = model_results[best_model_name]['accuracy']

    print(f"\n🏆 Best Model: {best_model_name} (Accuracy: {best_accuracy:.4f})")

    return best_model_name

best_model_name = analyze_model_performance(model_results)

# Save best_model.pkl
with open('best_model.pkl', 'wb') as f:
    pickle.dump(model_results[best_model_name]['model'], f)
print("💾 Saved best_model.pkl")

# Cell 12: Visualization Functions
def plot_model_comparison(model_results):
    """Plot model comparison charts"""
    model_names = list(model_results.keys())
    accuracies = [model_results[name]['accuracy'] for name in model_names]
    aucs = [model_results[name]['auc'] for name in model_names]

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # Accuracy comparison
    bars1 = ax1.barh(model_names, accuracies, color='skyblue')
    ax1.set_xlabel('Accuracy')
    ax1.set_title('Model Accuracy Comparison')
    ax1.grid(axis='x', alpha=0.3)

    # Add value labels on bars
    for bar, acc in zip(bars1, accuracies):
        ax1.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                 f'{acc:.3f}', ha='left', va='center')

    # AUC comparison
    bars2 = ax2.barh(model_names, aucs, color='lightcoral')
    ax2.set_xlabel('AUC Score')
    ax2.set_title('Model AUC Comparison')
    ax2.grid(axis='x', alpha=0.3)

    # Add value labels on bars
    for bar, auc in zip(bars2, aucs):
        ax2.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                 f'{auc:.3f}', ha='left', va='center')

    plt.tight_layout()
    plt.show()

def plot_confusion_matrix(model_results, model_name, y_test, le_target):
    """Plot confusion matrix for a specific model"""
    if model_name not in model_results:
        print(f"❌ Model {model_name} not found!")
        return

    y_pred = model_results[model_name]['predictions']
    cm = confusion_matrix(y_test, y_pred)

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=le_target.classes_,
                yticklabels=le_target.classes_)
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# Cell 13: Generate Visualizations
print("📊 Generating visualizations...")

# Plot model comparison
plot_model_comparison(model_results)

# Plot confusion matrix for best model
plot_confusion_matrix(model_results, best_model_name, y_test, le_target)

# Cell 14: Feature Importance (for best model if applicable)
def plot_feature_importance(model_results, best_model_name, feature_names):
    """Plot feature importance for tree-based models"""
    model = model_results[best_model_name]['model']

    if hasattr(model, 'feature_importances_'):
        print(f"📊 Feature Importance - {best_model_name}")

        feature_importance = pd.Series(model.feature_importances_, index=feature_names)
        top_features = feature_importance.sort_values(ascending=False).head(10)

        plt.figure(figsize=(10, 6))
        top_features.plot(kind='barh', color='lightgreen')
        plt.title(f'Top 10 Feature Importances - {best_model_name}')
        plt.xlabel('Importance Score')
        plt.grid(axis='x', alpha=0.3)
        plt.tight_layout()
        plt.show()

        return top_features
    else:
        print(f"❌ {best_model_name} doesn't support feature importance")
        return None

# Plot feature importance
top_features = plot_feature_importance(model_results, best_model_name, feature_names)

# Cell 15: Prediction Function
def predict_single_case(input_dict, model_results, le_dict, le_target, scaler, feature_names, model_name=None):
    """Predict a single case"""

    if model_name is None:
        model_name = max(model_results.keys(), key=lambda x: model_results[x]['accuracy'])

    if model_name not in model_results:
        print(f"❌ Model {model_name} not found!")
        return None

    print(f"🔮 Making prediction using {model_name}...")

    model = model_results[model_name]['model']

    # Convert input to DataFrame
    input_df = pd.DataFrame([input_dict])

    # Clean column names
    input_df.columns = input_df.columns.str.strip()

    # Encode categorical features
    for col in input_df.columns:
        if col in le_dict and input_df[col].dtype == 'object':
            try:
                input_df[col] = le_dict[col].transform(input_df[col])
            except ValueError as e:
                print(f"❌ Error encoding column '{col}': {e}")
                print(f"    Input value: {input_df[col].values[0]}")
                print(f"    Expected labels: {list(le_dict[col].classes_)}")
                return None

    # Ensure all features are present
    for col in feature_names:
        if col not in input_df.columns:
            input_df[col] = 0

    # Reorder columns to match training data
    input_df = input_df[feature_names]

    # Scale if needed
    if model_name in ["MLP", "SVM", "Logistic Regression", "KNN"]:
        input_df = scaler.transform(input_df)

    # Make prediction
    pred_class = model.predict(input_df)[0]
    pred_prob = model.predict_proba(input_df)[0]

    # Convert back to original labels
    predicted_result = le_target.inverse_transform([pred_class])[0]
    confidence = pred_prob[pred_class]

    # Print results
    print(f"\n🔬 PREDICTION RESULTS")
    print(f"{'='*50}")
    print(f"🎯 Predicted Result: {predicted_result}")
    print(f"🎯 Confidence: {confidence:.2%}")

    print(f"\n📊 Class-wise Probabilities:")
    for i, (cls, prob) in enumerate(zip(le_target.classes_, pred_prob)):
        print(f"    {cls}: {prob:.4f} ({prob:.2%})")

    return {
        'predicted_class': predicted_result,
        'confidence': confidence,
        'probabilities': dict(zip(le_target.classes_, pred_prob))
    }

# Cell 16: Test Prediction with Example Cases
print("🧪 Testing prediction with example cases...")

# Test case 1
test_case_1 = {
    'Age ': 4,
    'Breed species': 'Haryana  Hiefer',
    ' Sex ': 'F',
    'Calvings': 4,
    'Abortion History (Yes No)': 'No',
    'Infertility Repeat breeder(Yes No)': 'No',
    'Brucella vaccination status (Yes No)': 'No',
    'Sample Type(Serum Milk)': 'serum',
    'Test Type (RBPT ELISA MRT)': 'ELISA',
    'Retained Placenta Stillbirth(Yes No No Data)': 'No Data',
    'Proper Disposal of Aborted Fetuses (Yes No)': 'Yes'
}

print("📋 TEST CASE 1:")
prediction_1 = predict_single_case(test_case_1, model_results, le_dict, le_target, scaler, feature_names)

# Test case 2 (different parameters)
test_case_2 = {
    'Age ': 8,
    'Breed species': 'Haryana  Hiefer',
    ' Sex ': 'F',
    'Calvings': 2,
    'Abortion History (Yes No)': 'Yes',
    'Infertility Repeat breeder(Yes No)': 'Yes',
    'Brucella vaccination status (Yes No)': 'No',
    'Sample Type(Serum Milk)': 'serum',
    'Test Type (RBPT ELISA MRT)': 'ELISA',
    'Retained Placenta Stillbirth(Yes No No Data)': 'Yes',
    'Proper Disposal of Aborted Fetuses (Yes No)': 'No'
}

print("\n📋 TEST CASE 2:")
prediction_2 = predict_single_case(test_case_2, model_results, le_dict, le_target, scaler, feature_names)

# Cell 17: Compare Predictions Across All Models
def compare_predictions_across_models(input_dict, model_results, le_dict, le_target, scaler, feature_names):
    """Compare predictions across all models"""
    print(f"\n🔍 COMPARING PREDICTIONS ACROSS ALL MODELS")
    print(f"{'='*60}")

    results = {}

    for model_name in model_results.keys():
        try:
            result = predict_single_case(input_dict, model_results, le_dict, le_target, scaler, feature_names, model_name)
            if result:
                results[model_name] = result
                print(f"✅ {model_name:<20}: {result['predicted_class']:<10} ({result['confidence']:.2%})")
        except Exception as e:
            print(f"❌ {model_name:<20}: Error - {str(e)}")

    return results

print("\n🔄 Comparing predictions for Test Case 1 across all models:")
all_predictions = compare_predictions_across_models(test_case_1, model_results, le_dict, le_target, scaler, feature_names)

print("\n✅ All cells completed successfully!")
print("🎉 Your Brucellosis prediction model is ready to use!")

